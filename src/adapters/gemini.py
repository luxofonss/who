from __future__ import annotations

"""Google Gemini adapter using the official *google-generativeai* SDK.

Set the environment variable `GOOGLE_API_KEY` with your Gemini API key before
running the server:

    export GOOGLE_API_KEY="your-secret-key"

The model used is *gemini-1.5-flash* (fast & cost-effective).  Calls are
synchronous â€“ they run in a background thread when used inside FastAPI so the
event-loop is not blocked.
"""

import os
import json
from typing import Any, Dict, List, Optional

import google.generativeai as genai  # type: ignore
from loguru import logger
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks.manager import CallbackManagerForLLMRun


class Gemini:  # pylint: disable=too-few-public-methods
    """Thin wrapper around the Gemini SDK for a single shot prompt."""

    def __init__(
        self,
        *,
        model_name: str = "gemini-1.5-flash",
        temperature: float = 0.1,
    ) -> None:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise RuntimeError("GOOGLE_API_KEY environment variable is not set. Please set it with your Gemini API key.")

        # Configure the client once.  Idempotent.
        genai.configure(api_key=api_key)

        self._model = genai.GenerativeModel(model_name)
        self._generation_cfg: Dict[str, Any] = {
            "temperature": temperature,
        }
        self.model_name = model_name
        logger.info("Gemini adapter initialised (model=%s, temperature=%.2f)", model_name, temperature)

    # ------------------------------------------------------------------
    def invoke(self, prompt: str) -> str:
        """Send *prompt* to Gemini and return the text response.

        The prompt is logged at DEBUG level (up to 2 000 chars) to make tracing
        easy but avoid overwhelming the logs.
        """
        # logger.debug(f"Gemini prompt (first 100000 chars): {prompt[:100000]}")

        try:
            response = self._model.generate_content(
                [prompt],
                generation_config=self._generation_cfg,
            )
            text: str = response.text or ""
            # logger.debug(f"Gemini raw response: {text[:10000]}")
            return text
        except Exception as exc:  # noqa: BLE001
            logger.error(f"Gemini API call failed: {exc}")
            # Bubble up so caller can handle / return error JSON.
            raise 


class LangChainGemini(LLM):
    """LangChain-compatible wrapper around the Gemini class."""
    
    model_name: str = "gemini-1.5-flash"
    temperature: float = 0.1
    
    def __init__(
        self,
        *,
        model_name: str = "gemini-1.5-flash",
        temperature: float = 0.1,
        **kwargs
    ):
        super().__init__(model_name=model_name, temperature=temperature, **kwargs)
        # Create Gemini instance without setting it as a field to avoid Pydantic conflicts
        object.__setattr__(self, '_gemini', Gemini(model_name=model_name, temperature=temperature))

    @property
    def _llm_type(self) -> str:
        """Return identifier of llm type."""
        return "gemini"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Call out to Gemini's generate endpoint.
        
        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
            run_manager: Optional callback manager.
            
        Returns:
            The string generated by the model.
        """
        response = self._gemini.invoke(prompt)
        
        # Handle stop words if provided
        if stop:
            for stop_word in stop:
                if stop_word in response:
                    response = response.split(stop_word)[0]
                    break
        
        return response

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        return {
            "model_name": self.model_name,
            "temperature": self.temperature,
        } 