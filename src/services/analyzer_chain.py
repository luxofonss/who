from __future__ import annotations

import json
import asyncio
import re
from typing import Dict, List, Optional, Any, TypedDict, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path

from loguru import logger
from langchain_core.tools import Tool
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor

from adapters.gemini import Gemini, LangChainGemini
from services.retriever import LangChainRetriever
from services.prompt_builder import PromptBuilder


@dataclass
class AgentState(TypedDict):
    """State schema for LangGraph agent."""
    question: str
    context: str
    endpoint: str
    requirements: str
    testcases: str
    user_text: str
    history: List[str]
    retrieved_symbols: List[str]
    final_response: Optional[str]
    iteration_count: int
    seen_context: List[str]
    last_tool_call_symbols: List[str]
    new_retrieved_symbols: List[str]
    node_call_count: Dict[str, int] = field(default_factory=dict)


@dataclass
class AnalysisResult:
    """Structured result for endpoint analysis matching PromptBuilder schema."""
    document: str
    requirement_coverage: List[Dict[str, Any]]
    test_cases: List[Dict[str, Any]]
    improvements: List[Dict[str, str]]
    endpoint: str
    raw_response: Optional[str] = None
    analysis_method: str = "langgraph"  # "langgraph" or "fallback"


class AnalysisError(Exception):
    """Custom exception for analysis failures."""
    pass


class AnalyzerChain:
    """High-level orchestrator for endpoint analysis using LangGraph with multi-hop retrieval."""

    def __init__(self, project_id: str):
        if not project_id or not isinstance(project_id, str):
            raise ValueError("project_id must be a non-empty string")
        
        self.project_id = project_id
        self.retriever = LangChainRetriever(project_id)
        self.llm = Gemini(temperature=0)  # For fallback analysis
        self.langchain_llm = LangChainGemini(temperature=0)  # For LangGraph agent
        
        # Create tool and graph
        self._setup_langgraph()
        
        logger.info(f"ðŸš€ AnalyzerChain initialized with LangGraph for project: {project_id}")

    def _setup_langgraph(self):
        """Initialize LangGraph components."""
        logger.info("ðŸ”§ Setting up LangGraph components...")
        
        # Create the tool
        self.get_context_tool = Tool(
            name="get_project_code_context",
            func=self._find_symbol_context,
            description=(
                "Return code content related to any symbol (class/method/DTO/service/etc) from the project. "
                "Use it when you need more details about classes, methods, or components mentioned in the code. "
                "Pass the exact name of the class, interface, method, or component you want to understand better."
            )
        )
        logger.info("âœ… Created get_project_code_context tool")
        
        # Create tool executor
        self.tool_executor = ToolExecutor([self.get_context_tool])
        logger.info("âœ… Created tool executor")
        
        # Build the graph
        logger.info("ðŸ—ï¸ Building LangGraph workflow...")
        self._build_graph()
        logger.info("âœ… LangGraph setup complete")

    def _write_prompt_to_file(self, prompt: str, prefix: str = "prompt") -> str:
        """Write prompt to a file for debugging/review purposes."""
        try:
            prompts_dir = Path("logs/prompts")
            prompts_dir.mkdir(parents=True, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{prefix}_{self.project_id}_{timestamp}.txt"
            filepath = prompts_dir / filename
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"Project: {self.project_id}\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Type: {prefix}\n")
                f.write("=" * 80 + "\n\n")
                f.write(prompt)
            logger.info(f"ðŸ“ Prompt saved to: {filepath}")
            return str(filepath)
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to save prompt to file: {e}")
            return ""

    def _write_response_to_file(self, response: str, iteration: int) -> str:
        """Write agent response to a file for debugging/review purposes."""
        try:
            responses_dir = Path("logs/responses")
            responses_dir.mkdir(parents=True, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"response_iteration_{iteration}_{self.project_id}_{timestamp}.txt"
            filepath = responses_dir / filename
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"Project: {self.project_id}\n")
                f.write(f"Iteration: {iteration}\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write("=" * 80 + "\n\n")
                f.write(response)
            logger.info(f"ðŸ“ Response saved to: {filepath}")
            return str(filepath)
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to save response to file: {e}")
            return ""

    def _build_graph(self):
        """Build the LangGraph workflow."""
        graph = StateGraph(AgentState)

        # Add nodes
        graph.add_node("agent", self._agent_node)
        graph.add_node("use_tool", self._call_tool_node)
        graph.add_node("verify", self._verify_response_node)

        # Agent: decide whether to use tool or go verify
        graph.add_conditional_edges(
            "agent",
            self._should_use_tool,
            {
                "use_tool": "use_tool",
                "end": "verify"
            }
        )

        # Tool: after tool call, go back to agent
        graph.add_edge("use_tool", "agent")

        # Verifier: decide whether to loop back or end
        def _verify_condition(state: AgentState) -> str:
            """Determine if verification should loop back to agent or end."""
            logger.debug(f"ðŸ” Verifying state: iteration {state['iteration_count']}, final_response: {state['final_response'][:100] if state['final_response'] else 'None'}...")
            
            # Check iteration limit
            if state["iteration_count"] >= 5:
                logger.warning("âš ï¸ Max iterations reached in verify node, forcing end")
                return "end"

            # Check if no new symbols or chunks were retrieved in the last tool call
            if state.get("last_tool_call_symbols") and not state.get("new_retrieved_symbols") and not state.get("new_retrieved_chunks"):
                logger.info("ðŸ No new context or symbols retrieved in last tool call, ending workflow")
                return "end"

            # Check if response is valid JSON
            response = state["final_response"] or ""
            if response.strip().startswith("{") and response.strip().endswith("}"):
                try:
                    json.loads(response)
                    logger.info("âœ… Valid JSON response in verify node, proceeding to end")
                    return "end"
                except json.JSONDecodeError:
                    logger.warning("âš ï¸ Response looks like JSON but is invalid")

            # If final_response is None or verification marked it incomplete, loop back
            if state["final_response"] is None:
                logger.info("ðŸ” Verification incomplete, looping back to agent")
                return "agent"

            # Default to end if no clear reason to loop
            logger.info("ðŸ No conditions for looping, ending workflow")
            return "end"

        graph.add_conditional_edges(
            "verify",
            _verify_condition,
            {
                "agent": "agent",
                "end": END
            }
        )

        # Set entry point
        graph.set_entry_point("agent")

        # Compile the graph
        self.graph = graph.compile()
        logger.info("âœ… LangGraph workflow compiled with enhanced verification")

    def _find_symbol_context(self, symbol: str, seen_chunks: List[str]) -> Tuple[str, List[str]]:
        """Helper method to find code context for a given symbol, excluding seen chunks."""
        logger.info(f"ðŸ” Searching for symbol: '{symbol}'")
        try:
            # Try direct symbol search first
            logger.debug(f"ðŸŽ¯ Attempting direct symbol lookup for: {symbol}")
            docs = self.retriever.find_by_symbol_name(symbol)
            
            # If no direct match, try a broader search
            if not docs:
                logger.debug(f"ðŸ”„ No direct match for '{symbol}', trying semantic search...")
                docs = self.retriever.retrieve_sync(
                    symbol,
                    user_text="",
                    top=3,
                    hyde=False
                )
            
            if docs:
                new_chunks = []
                new_chunk_ids = []
                for doc in docs:
                    chunk_id = doc.metadata.get("id", str(hash(doc.page_content)))  # Fallback to hash if no chunk_id
                    if chunk_id not in seen_chunks:
                        new_chunks.append(doc.page_content)
                        new_chunk_ids.append(chunk_id)
                        logger.debug(f"âœ… Added chunk {chunk_id} for symbol '{symbol}'")
                    else:
                        logger.debug(f"â­ï¸ Skipped already seen chunk {chunk_id} for symbol '{symbol}'")
                
                if new_chunks:
                    result = "\n\n".join(new_chunks)
                    logger.info(f"âœ… Found {len(new_chunks)} new documents for '{symbol}' ({len(result)} chars)")
                    logger.debug(f"ðŸ“„ Context preview for '{symbol}': {result[:150]}...")
                    return f"'{symbol}':\n{result}", new_chunk_ids
                else:
                    logger.warning(f"âŒ No new chunks found for symbol: '{symbol}'")
                    return f"No new code found for symbol: {symbol}.", []
            else:
                logger.warning(f"âŒ No code found for symbol: '{symbol}'")
                return f"No code found for symbol: {symbol}. Try a different class or method name.", []
        except Exception as e:
            logger.error(f"ðŸ’¥ Error retrieving context for symbol '{symbol}': {str(e)}")
            return f"Error retrieving code for symbol: {symbol} - {str(e)}", []

    def _agent_node(self, state: AgentState) -> AgentState:
        """Agent reasoning node."""
        node_name = "agent"
        state["node_call_count"][node_name] = state["node_call_count"].get(node_name, 0) + 1
        logger.info(f"ðŸ¤– Agent iteration {state['iteration_count']} - analyzing endpoint: {state['endpoint']}")
        logger.debug(f"ðŸ“Š Current context length: {len(state['context'])} chars")
        logger.debug(f"ðŸ” Retrieved symbols so far: {state['retrieved_symbols']}")
        
        # Build the analysis prompt
        prompt = f"""You are an expert software architect analyzing the REST endpoint: {state['endpoint']}
            ANALYSIS STRATEGY:
            1. Review the current context for the endpoint implementation
            2. Get all classes, services, repositories, DTOs, methods and check if they are fully implemented
            3. If you see references to any classes, services, DTOs, or methods that are not fully shown, request more context using the tool
            4. When there are no classes, services, DTOs, or methods that are not fully shown, you have sufficient implementation details, provide your final analysis as JSON
            5. Do not assume that you have enough context, always check the code and use get_project_code_context if any part of the code is not fully implemented.

            WHEN TO USE get_project_code_context TOOL:
            - When you see class/interface names without their implementation
            - When service methods are referenced but not shown
            - When DTO/model classes are mentioned but structure is unclear
            - When exception handling classes are referenced
            - When you need to understand dependencies or business logic

            To use the tool, respond with: "I need to get context for [exact_class_or_method_name]"
            Examples:
            - "I need to get context for UserService"
            - "I need to get context for ValidationException"
            - "I need to get context for OrderDto"

            CURRENT CONTEXT:
            {state['context']}

            REQUIREMENTS TO ANALYZE:
            {state['requirements']}

            TEST CASES TO VERIFY:
            {state['testcases']}

            ADDITIONAL INSTRUCTIONS:
            {state['user_text']}

            If you don't have enough context, call the get_project_code_context tool to get more context, don't assume.
            If you have enough context, provide your final analysis as valid JSON with this structure:
            {{
                "document": "detailed explanation of what the endpoint does",
                "requirement_coverage": [
                    {{
                        "requirement": "exact requirement text",
                        "coverage_score": "0-100",
                        "explain": "how the code meets or fails this requirement"
                    }}
                ],
                "test_cases": [
                    {{
                        "test_case": "exact test case text",
                        "coverage_score": "0-100", 
                        "explain": "whether this test case is covered by the implementation"
                    }}
                ],
                "improvements": [
                    {{
                        "type": "category",
                        "reason": "what needs improvement",
                        "solution": "recommended fix"
                    }}
                ]
            }}

            Do not assume any code logic, always check the code and use get_project_code_context if any part of the code is not fully implemented.
            Your response:"""
        
        try:
            logger.debug("ðŸ”„ Calling LLM for agent reasoning...")
            prompt_file = self._write_prompt_to_file(prompt, f"agent_iteration_{state['iteration_count']}")
            logger.info(f"ðŸ’¾ Prompt saved to file: {prompt_file}")
            response = self.langchain_llm._call(prompt)
            logger.info(f"âœ… Agent response received (length: {len(response)} chars)")
            response_file = self._write_response_to_file(response, state['iteration_count'])
            logger.info(f"ðŸ’¾ Response saved to file: {response_file}")
            
            return {
                **state,
                "final_response": response,
                "history": state["history"] + [response],
                "iteration_count": state["iteration_count"] + 1,
                "node_call_count": state["node_call_count"]
            }
        except Exception as e:
            logger.error(f"âŒ Agent node error: {str(e)}")
            return {
                **state,
                "final_response": f"Error in agent reasoning: {str(e)}",
                "iteration_count": state["iteration_count"] + 1,
                "node_call_count": state["node_call_count"]
            }

    def _should_use_tool(self, state: AgentState) -> str:
        """Determine if the agent needs to use a tool."""
        response = state["final_response"] or ""
        iteration = state["iteration_count"]
        logger.debug(f"ðŸ¤” Decision time - iteration {iteration}, response length: {len(response)}")
        
        # Check if response is valid JSON (final answer)
        response_clean = response.strip()
        if response_clean.startswith("{") and response_clean.endswith("}"):
            try:
                json.loads(response_clean)
                logger.info("âœ… Valid JSON response - ending workflow")
                return "end"
            except json.JSONDecodeError:
                logger.warning("âš ï¸ Response looks like JSON but is invalid")

        # Check for explicit tool request
        if "I need to get context for" in response or "get_project_code_context" in response:
            logger.info("ðŸ”§ Agent explicitly requested tool usage")
            return "use_tool"

        # Stop if max iterations reached
        if iteration >= 5:
            logger.warning(f"âš ï¸ Max iterations ({iteration}) reached, forcing end")
            return "end"

        # Check if no new symbols or chunks were retrieved in the last tool call
        if state.get("last_tool_call_symbols") and not state.get("new_retrieved_symbols") and not state.get("new_retrieved_chunks"):
            logger.info("ðŸ No new context retrieved in last tool call - ending workflow")
            return "end"

        # Narrow down keyword-based tool triggering
        specific_patterns = [
            r"\b(?:need to inspect|examine|check|see)\s+([A-Z][A-Za-z0-9]*(?:Dto|Service|Controller|Repository|Entity|Exception))\b",
            r"\bimplementation of\s+([A-Z][A-Za-z0-9]*(?:Dto|Service|Controller|Repository|Entity|Exception))\b",
        ]
        for pattern in specific_patterns:
            matches = re.findall(pattern, response, re.IGNORECASE)
            if matches:
                logger.info(f"ðŸ”§ Found specific symbol references: {matches}")
                return "use_tool"

        logger.info("ðŸ No clear need for tool - proceeding to verification")
        return "end"

    def _get_context_for_symbols(self, symbols: List[str], already_retrieved: List[str], seen_chunks: List[str]) -> Tuple[str, List[str], List[str]]:
        """Fetch and return new context, retrieved symbols, and new chunk IDs."""
        new_context_parts = []
        new_retrieved = []
        new_chunk_ids = []
        for symbol in symbols:
            if symbol not in already_retrieved:
                logger.info(f"ðŸ” Fetching context for: {symbol}")
                context, chunk_ids = self._find_symbol_context(symbol, seen_chunks)
                if "No code found" not in context and "Error retrieving code" not in context:
                    new_context_parts.append(context)
                    new_retrieved.append(symbol)
                    new_chunk_ids.extend(chunk_ids)
                    logger.info(f"âœ… Successfully retrieved context for: {symbol} (chunks: {chunk_ids})")
                else:
                    logger.warning(f"âŒ No context found for: {symbol}")
            else:
                logger.debug(f"â­ï¸ Skipping already retrieved symbol: {symbol}")
        return "\n\n".join(new_context_parts), new_retrieved, new_chunk_ids

    def _call_tool_node(self, state: AgentState) -> AgentState:
        """Tool calling node - intelligently extract symbols from context and agent response."""
        logger.info("ðŸ”§ Tool node activated - extracting symbols to fetch")
        response = state["final_response"] or ""
        current_context = state["context"]
        already_retrieved = state["retrieved_symbols"]
        seen_chunks = state.get("seen_context", [])
        symbols = []

        # If missing_symbols from verification, prioritize those
        if state.get("missing_symbols"):
            symbols = [s for s in state["missing_symbols"] if s not in already_retrieved]
            logger.info(f"ðŸ” Fetching context for missing symbols from verification: {symbols}")
        else:
            explicit_patterns = [
                r"I need to get context for ([A-Za-z][A-Za-z0-9_]*)",
                r"get_project_code_context\([\"']([^\"']+)[\"']\)",
                r"(?:context for|details about|information on) ([A-Za-z][A-Za-z0-9_]*)",
            ]
            for pattern in explicit_patterns:
                matches = re.findall(pattern, response, re.IGNORECASE)
                if matches:
                    flattened_matches = []
                    for match in matches:
                        if isinstance(match, tuple):
                            flattened_matches.extend([m for m in match if m])
                        else:
                            flattened_matches.append(match)
                    symbols.extend(flattened_matches)
                    logger.info(f"ðŸŽ¯ Found explicit symbol requests: {flattened_matches}")

            symbols = [s for s in symbols if s not in already_retrieved]
            if not symbols:
                logger.info("ðŸ” Looking for symbols mentioned in 'need more context' statements...")
                inspection_patterns = [
                    r"(?:need to inspect|we need to inspect|inspect)\s+`([A-Za-z][A-Za-z0-9_]*)`",
                    r"(?:need to examine|we need to examine|examine)\s+`([A-Za-z][A-Za-z0-9_]*)`", 
                    r"(?:implementation of|depends on the implementation of)\s+`([A-Za-z][A-Za-z0-9_]*)`",
                    r"(?:structure of|details of)\s+`([A-Za-z][A-Za-z0-9_]*)`",
                    r"(?:need to check|we need to check|check)\s+`([A-Za-z][A-Za-z0-9_]*)`",
                    r"(?:need to see|we need to see|see)\s+`([A-Za-z][A-Za-z0-9_]*)`",
                    r"(?:requires inspection of|inspection of)\s+`([A-Za-z][A-Za-z0-9_]*)`",
                    r"(?:need to inspect|we need to inspect|inspect)\s+([A-Z][A-Za-z0-9]*(?:Dto|Service|Controller|Repository|Response|Request|Entity|Exception))",
                    r"(?:implementation of|depends on the implementation of)\s+([A-Z][A-Za-z0-9]*(?:Dto|Service|Controller|Repository|Response|Request|Entity|Exception))",
                    r"(?:structure of|details of)\s+([A-Z][A-Za-z0-9]*(?:Dto|Service|Controller|Repository|Response|Request|Entity|Exception))",
                ]
                for pattern in inspection_patterns:
                    matches = re.findall(pattern, response, re.IGNORECASE)
                    if matches:
                        symbols.extend(matches)
                        logger.info(f"ðŸŽ¯ Found symbols in inspection context: {matches}")
            if not symbols:
                logger.info("ðŸ” No explicit requests - analyzing context for missing implementations...")
                symbols = self._extract_missing_symbols(current_context, already_retrieved)
            if not symbols:
                logger.info("ðŸ”Ž Scanning for any class/service references...")
                symbols = self._find_any_symbols(current_context, already_retrieved)

        if not symbols:
            logger.warning("âš ï¸ No symbols identified - ending tool usage")
            return {
                **state,
                "final_response": None,
                "last_tool_call_symbols": [],
                "new_retrieved_symbols": [],
                "new_retrieved_chunks": []
            }

        symbols = [s for s in symbols if s not in already_retrieved][:10]
        logger.info(f"ðŸ“‹ Found {len(symbols)} symbols to investigate: {symbols}")
        new_context, new_retrieved, new_chunk_ids = self._get_context_for_symbols(symbols, already_retrieved, seen_chunks)
        updated_context = state["context"]
        if new_context:
            updated_context += "\n\n" + new_context
            logger.info(f"ðŸ“ Added {len(new_retrieved)} new context sections, {len(new_chunk_ids)} new chunks")
        
        return {
            **state,
            "context": updated_context,
            "retrieved_symbols": state["retrieved_symbols"] + new_retrieved,
            "seen_context": state["seen_context"] + new_chunk_ids,
            "final_response": None,
            "last_tool_call_symbols": symbols,
            "new_retrieved_symbols": new_retrieved,
            "new_retrieved_chunks": new_chunk_ids
        }

    def _extract_missing_symbols(self, context: str, already_retrieved: List[str]) -> List[str]:
        """Extract symbols that are referenced but not fully implemented in context."""
        import re
        symbols = []
        patterns = [
            r"(\w+(?:Service|Repository|Controller|Dto|Entity|Exception))\b(?!\s*\{)",
            r"new\s+(\w+(?:Service|Repository|Controller|Dto|Entity|Exception))\b",
            r"\b(\w+(?:Service|Repository|Controller|Dto|Entity|Exception))\.\w+\b",
        ]
        for pattern in patterns:
            matches = re.findall(pattern, context)
            for match in matches:
                symbol = match if isinstance(match, str) else match[0]
                if (symbol and symbol not in already_retrieved and len(symbol) > 3 and
                    symbol not in ['String', 'List', 'Map', 'Set', 'Boolean', 'Integer', 'Long', 'Date', 'Time']):
                    symbols.append(symbol)
        unique_symbols = list(dict.fromkeys(symbols))[:3]
        logger.debug(f"ðŸ” Extracted potential symbols: {unique_symbols}")
        return unique_symbols

    def _find_any_symbols(self, context: str, already_retrieved: List[str]) -> List[str]:
        """Find any class names that might be worth investigating."""
        import re
        symbols = []
        class_pattern = r'\b([A-Z][A-Za-z0-9]*(?:Service|Repository|Controller|Dto|Entity|Exception))\b(?!\s*\{)'
        matches = re.findall(class_pattern, context, re.IGNORECASE)
        for match in matches:
            if (len(match) > 4 and
                match not in already_retrieved and
                match not in ['String', 'Object', 'List', 'Map', 'Set', 'Boolean', 'Integer', 'Long', 'Date', 'Time']):
                symbols.append(match)
        unique_symbols = list(dict.fromkeys(symbols))[:2]
        logger.debug(f"ðŸ”Ž Found general class references: {unique_symbols}")
        return unique_symbols

    def _validate_inputs(self, endpoint: str, requirements_txt: str, testcases_txt: str, user_text: str) -> None:
        """Validate input parameters."""
        if not endpoint or not isinstance(endpoint, str):
            raise ValueError("endpoint must be a non-empty string")
        for param_name, param_value in [
            ("requirements_txt", requirements_txt),
            ("testcases_txt", testcases_txt), 
            ("user_text", user_text)
        ]:
            if not isinstance(param_value, str):
                raise ValueError(f"{param_name} must be a string")

    def _parse_json_response(self, response_text: str) -> str:
        import re
        match = re.search(r"```json\s*([\s\S]+?)\s*```", response_text)
        if match:
            return match.group(1).strip()
        match = re.search(r"```\s*([\s\S]+?)\s*```", response_text)
        if match:
            return match.group(1).strip()
        match = re.search(r"\{[\s\S]+\}", response_text)
        if match:
            return match.group(0).strip()
        return response_text.strip()

    def _parse_graph_response(self, graph_response: str, endpoint: str) -> AnalysisResult:
        """Parse LangGraph response and create structured result."""
        response_text = graph_response.strip()
        logger.info(f"ðŸ” Graph response: {response_text[:200]}...")
        response_text = self._parse_json_response(response_text)
        json_match = re.search(r'\{.*?"document".*?\}', response_text, re.DOTALL)
        if json_match and not response_text.strip().startswith('{'):
            response_text = json_match.group(0)
        
        try:
            result_dict = json.loads(response_text)
            logger.info("âœ… LangGraph returned valid JSON analysis")
            return AnalysisResult(
                document=result_dict.get("document", ""),
                requirement_coverage=result_dict.get("requirement_coverage", []),
                test_cases=result_dict.get("test_cases", []),
                improvements=result_dict.get("improvements", []),
                endpoint=endpoint,
                analysis_method="langgraph"
            )
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ LangGraph response is not valid JSON: {str(e)[:100]}...")
            logger.debug(f"Raw graph response: {response_text[:500]}...")
            return AnalysisResult(
                document="Analysis completed but not in JSON format",
                requirement_coverage=[],
                test_cases=[],
                improvements=[],
                endpoint=endpoint,
                raw_response=graph_response,
                analysis_method="langgraph"
            )

    async def run(
            self,
            *,
            endpoint: str,
            requirements_txt: str,
            testcases_txt: str,
            user_text: str,
    ) -> Dict[str, Any]:
        """Run the LangGraph analysis chain and return structured results."""
        try:
            self._validate_inputs(endpoint, requirements_txt, testcases_txt, user_text)
        except ValueError as e:
            logger.error(f"âŒ Input validation failed: {str(e)}")
            raise AnalysisError(f"Invalid input: {str(e)}")
        
        logger.info(f"ðŸš€ Starting LangGraph AnalyzerChain for endpoint: {endpoint}")
        logger.info(f"ðŸ“‹ Requirements: {len(requirements_txt)} chars")
        logger.info(f"ðŸ§ª Test cases: {len(testcases_txt)} chars")
        logger.info(f"ðŸ’¬ User instructions: {len(user_text)} chars")
        
        try:
            logger.info("ðŸ” Step 1: Retrieving initial context from vector database...")
            docs = await self.retriever.retrieve(endpoint, user_text, top=3, hyde=False)
            initial_context = "\n\n".join(doc.page_content for doc in docs)
            initial_chunk_ids = [doc.metadata.get("id", str(hash(doc.page_content))) for doc in docs]
            logger.info(f"âœ… Retrieved {len(docs)} initial documents ({len(initial_context)} chars total)")
            logger.debug(f"ï¿½ Ascociated chunk IDs: {initial_chunk_ids}")
            
            logger.info("ðŸ—ï¸ Step 2: Creating initial state for LangGraph workflow...")
            initial_state: AgentState = {
                "question": f"Analyze the REST endpoint '{endpoint}' according to the requirements and test cases.",
                "context": initial_context,
                "endpoint": endpoint,
                "requirements": requirements_txt,
                "testcases": testcases_txt,
                "user_text": user_text,
                "history": [],
                "retrieved_symbols": [],
                "seen_context": initial_chunk_ids,
                "final_response": None,
                "iteration_count": 0,
                "last_tool_call_symbols": [],
                "new_retrieved_symbols": [],
                "new_retrieved_chunks": [],
                "node_call_count": {}
            }
            logger.debug("âœ… Initial state created successfully")
            
            logger.info("ðŸš€ Step 3: Starting LangGraph analysis workflow...")
            final_state = await asyncio.to_thread(self.graph.invoke, initial_state)
            
            iterations = final_state['iteration_count']
            retrieved_symbols = final_state['retrieved_symbols']
            final_response_length = len(final_state.get("final_response", ""))
            logger.info(f"ðŸŽ‰ LangGraph workflow completed successfully!")
            logger.info(f"ðŸ“Š Statistics: {iterations} iterations, {len(retrieved_symbols)} symbols retrieved")
            logger.info(f"ðŸ” Retrieved context for symbols: {retrieved_symbols}")
            logger.info(f"ðŸ“ Final response length: {final_response_length} chars")
            
            logger.info("ðŸ”§ Step 4: Parsing and structuring final response...")
            final_response = final_state.get("final_response", "")
            result = self._parse_graph_response(final_response, endpoint)
            logger.info(f"âœ… Analysis complete - method: {result.analysis_method}")
            return result.__dict__
            
        except AnalysisError:
            raise
        except Exception as e:
            logger.error(f"âŒ LangGraph analysis failed: {str(e)}")
            try:
                return await self._fallback_analysis(
                    endpoint=endpoint,
                    requirements_txt=requirements_txt,
                    testcases_txt=testcases_txt,
                    user_text=user_text,
                    initial_context=initial_context if 'initial_context' in locals() else ""
                )
            except Exception as fallback_error:
                logger.error(f"âŒ Fallback analysis also failed: {str(fallback_error)}")
                raise AnalysisError(f"Both LangGraph and fallback analysis failed. LangGraph error: {str(e)}, Fallback error: {str(fallback_error)}")

    async def _fallback_analysis(
            self,
            *,
            endpoint: str,
            requirements_txt: str,
            testcases_txt: str,
            user_text: str,
            initial_context: str
    ) -> Dict[str, Any]:
        """Fallback to original analysis approach if agent fails."""
        logger.info("ðŸ”„ Using fallback analysis method")
        try:
            prompt = PromptBuilder.build_analysis_prompt(
                endpoint=endpoint,
                context=initial_context,
                requirements=requirements_txt,
                testcases=testcases_txt,
                user_text=user_text,
            )
            fallback_prompt_file = self._write_prompt_to_file(prompt, "fallback_analysis")
            logger.info(f"ðŸ’¾ Fallback prompt saved to: {fallback_prompt_file}")
            resp = await asyncio.to_thread(self.llm.invoke, prompt)
            fallback_response_file = self._write_response_to_file(resp, 0)
            logger.info(f"ðŸ’¾ Fallback response saved to: {fallback_response_file}")
            try:
                result_dict = json.loads(resp)
                logger.info("âœ… Fallback analysis returned valid JSON")
                return AnalysisResult(
                    document=result_dict.get("document", ""),
                    requirement_coverage=result_dict.get("requirement_coverage", []),
                    test_cases=result_dict.get("test_cases", []),
                    improvements=result_dict.get("improvements", []),
                    endpoint=endpoint,
                    analysis_method="fallback"
                ).__dict__
            except json.JSONDecodeError:
                logger.warning("âš ï¸ Fallback analysis failed to return JSON")
                return AnalysisResult(
                    document="Fallback analysis completed but not in JSON format",
                    requirement_coverage=[],
                    test_cases=[],
                    improvements=[],
                    endpoint=endpoint,
                    raw_response=resp,
                    analysis_method="fallback"
                ).__dict__
        except Exception as e:
            logger.error(f"âŒ Fallback analysis execution failed: {str(e)}")
            raise AnalysisError(f"Fallback analysis failed: {str(e)}")

    def _verify_response_node(self, state: AgentState) -> AgentState:
        """Verify final LLM response before ending."""
        node_name = "verify"
        state["node_call_count"][node_name] = state["node_call_count"].get(node_name, 0) + 1
        if state["node_call_count"][node_name] > 3:
            logger.warning("Max iterations reached, ending workflow to prevent infinite loop.")
            return state

        response = state["final_response"] or ""
        prompt = f"""
            You are a verification assistant. A REST API endpoint has been analyzed.
            Your job is to:
            1. Review the response and determine if it fully covers the user requirements and test cases.
            2. List any classes, DTOs, methods, configs, or components in business logic that are referenced in the requirements/test cases but are missing in the context or response.
            3. If complete, return: {{"status": "complete"}}
            4. If incomplete, return: 
            {{
                "status": "incomplete",
                "missing_symbols": ["ClassA", "SomeDto", "MyService.methodX"]
            }}

            RESPONSE:
            {response}

            REQUIREMENTS:
            {state['requirements']}

            TEST CASES:
            {state['testcases']}

            CONTEXT:
            {state['context']}

            ALREADY RETRIEVED SYMBOLS:
            {state['retrieved_symbols']}

            ALREADY SEEN CHUNK IDS:
            {state['seen_context']}
            """

        try:
            logger.info(f"ðŸ” Verifier prompt: {prompt}")
            result = self.llm.invoke(prompt)
            logger.info(f"ðŸ•µï¸â€â™€ï¸ Verification result: {result}")
            parsed = self._parse_json_response(result)
            parsed = json.loads(parsed)

            if parsed.get("status") == "complete":
                logger.info("âœ… Verifier accepted final response â€” ending workflow")
                return state

            missing_symbols = parsed.get("missing_symbols", [])
            missing_symbols = [s for s in missing_symbols if s not in state["retrieved_symbols"]]
            if not missing_symbols:
                logger.info("âœ… No new missing symbols â€” ending workflow")
                return state

            logger.info(f"ðŸ” Verifier found missing symbols: {missing_symbols}")
            context_add = "\n\n".join([self._find_symbol_context(s, state["seen_context"])[0] for s in missing_symbols])
            new_chunk_ids = []
            for s in missing_symbols:
                _, chunk_ids = self._find_symbol_context(s, state["seen_context"])
                new_chunk_ids.extend(chunk_ids)

            return {
                **state,
                "context": state["context"] + "\n\n" + context_add,
                "retrieved_symbols": state["retrieved_symbols"] + missing_symbols,
                "seen_context": state["seen_context"] + new_chunk_ids,
                "final_response": None,
            }
        except Exception as e:
            logger.warning(f"âš ï¸ Verifier failed to parse or reason: {e}")
            return state

    def clear_cache(self) -> None:
        """Clear any cached resources (LangGraph doesn't require caching)."""
        logger.info("ðŸ§¹ LangGraph resources cleared (no caching needed)")